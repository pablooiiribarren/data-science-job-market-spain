{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727d9479",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# %% [markdown]\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# # üßπ Limpieza y Procesamiento de Datos\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ## An√°lisis del Mercado Laboral de Data Science en Espa√±a\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # üßπ Limpieza y Procesamiento de Datos\n",
    "# ## An√°lisis del Mercado Laboral de Data Science en Espa√±a\n",
    "# \n",
    "# **Objetivo:** Limpiar y preparar los datos crudos para an√°lisis\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuraci√≥n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# %%\n",
    "# Cargar los datos m√°s recientes\n",
    "data_dir = Path(\"../data/raw\")\n",
    "latest_file = sorted(data_dir.glob(\"jobs_data_*.csv\"))[-1]\n",
    "print(f\"üìÇ Cargando: {latest_file.name}\")\n",
    "\n",
    "df_raw = pd.read_csv(latest_file)\n",
    "print(f\"‚úÖ {len(df_raw)} ofertas cargadas\")\n",
    "df_raw.head()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1Ô∏è‚É£ Inspecci√≥n Inicial\n",
    "\n",
    "# %%\n",
    "# Dimensiones\n",
    "print(f\"üìä Dimensiones: {df_raw.shape[0]} filas √ó {df_raw.shape[1]} columnas\\n\")\n",
    "\n",
    "# Info general\n",
    "df_raw.info()\n",
    "\n",
    "# %%\n",
    "# Valores nulos\n",
    "print(\"üîç Valores nulos por columna:\\n\")\n",
    "missing = df_raw.isnull().sum()\n",
    "missing_pct = (missing / len(df_raw) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Nulos': missing,\n",
    "    'Porcentaje': missing_pct\n",
    "})\n",
    "print(missing_df[missing_df['Nulos'] > 0].sort_values('Nulos', ascending=False))\n",
    "\n",
    "# %%\n",
    "# Duplicados\n",
    "duplicates = df_raw.duplicated(subset=['id']).sum()\n",
    "print(f\"üîÅ Ofertas duplicadas por ID: {duplicates}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2Ô∏è‚É£ Limpieza de Datos\n",
    "\n",
    "# %%\n",
    "# Crear copia para trabajar\n",
    "df = df_raw.copy()\n",
    "\n",
    "# %%\n",
    "# 2.1 Eliminar duplicados\n",
    "df = df.drop_duplicates(subset=['id'], keep='first')\n",
    "print(f\"‚úÖ Duplicados eliminados: {len(df_raw) - len(df)}\")\n",
    "print(f\"üìä Ofertas √∫nicas: {len(df)}\")\n",
    "\n",
    "# %%\n",
    "# 2.2 Limpiar y estandarizar ubicaciones\n",
    "def clean_location(location):\n",
    "    \"\"\"Limpia y estandariza nombres de ubicaciones\"\"\"\n",
    "    if pd.isna(location):\n",
    "        return 'Desconocido'\n",
    "    \n",
    "    location = str(location).strip()\n",
    "    \n",
    "    # Extraer ciudad principal (antes de la coma)\n",
    "    if ',' in location:\n",
    "        location = location.split(',')[0].strip()\n",
    "    \n",
    "    # Estandarizar nombres comunes\n",
    "    replacements = {\n",
    "        'Donostia': 'San Sebasti√°n',\n",
    "        'Donostia-San Sebasti√°n': 'San Sebasti√°n',\n",
    "        'A Coru√±a': 'La Coru√±a',\n",
    "        'Zaragoza': 'Zaragoza',\n",
    "        'Vizcaya': 'Bilbao',\n",
    "        'Guip√∫zcoa': 'San Sebasti√°n'\n",
    "    }\n",
    "    \n",
    "    for old, new in replacements.items():\n",
    "        if old in location:\n",
    "            location = new\n",
    "    \n",
    "    return location\n",
    "\n",
    "df['city'] = df['location_display'].apply(clean_location)\n",
    "print(f\"‚úÖ Ubicaciones procesadas\")\n",
    "print(f\"üìç Ciudades √∫nicas: {df['city'].nunique()}\")\n",
    "\n",
    "# %%\n",
    "# Top 10 ciudades\n",
    "print(\"\\nüèôÔ∏è Top 10 ciudades con m√°s ofertas:\")\n",
    "print(df['city'].value_counts().head(10))\n",
    "\n",
    "# %%\n",
    "# 2.3 Clasificar nivel de experiencia desde el t√≠tulo\n",
    "def extract_seniority(title):\n",
    "    \"\"\"Extrae el nivel de experiencia del t√≠tulo\"\"\"\n",
    "    if pd.isna(title):\n",
    "        return 'No especificado'\n",
    "    \n",
    "    title_lower = str(title).lower()\n",
    "    \n",
    "    if any(word in title_lower for word in ['junior', 'jr', 'trainee', 'graduate', 'entry']):\n",
    "        return 'Junior'\n",
    "    elif any(word in title_lower for word in ['senior', 'sr', 'lead', 'principal', 'staff']):\n",
    "        return 'Senior'\n",
    "    elif any(word in title_lower for word in ['manager', 'head', 'director', 'chief']):\n",
    "        return 'Manager'\n",
    "    else:\n",
    "        return 'Mid-Level'\n",
    "\n",
    "df['seniority'] = df['title'].apply(extract_seniority)\n",
    "print(\"\\nüìä Distribuci√≥n de niveles de experiencia:\")\n",
    "print(df['seniority'].value_counts())\n",
    "\n",
    "# %%\n",
    "# 2.4 Categorizar tipos de roles\n",
    "def categorize_role(title):\n",
    "    \"\"\"Categoriza el tipo de rol de data\"\"\"\n",
    "    if pd.isna(title):\n",
    "        return 'Otros'\n",
    "    \n",
    "    title_lower = str(title).lower()\n",
    "    \n",
    "    if 'scientist' in title_lower:\n",
    "        return 'Data Scientist'\n",
    "    elif 'analyst' in title_lower or 'analytics' in title_lower:\n",
    "        return 'Data Analyst'\n",
    "    elif 'engineer' in title_lower and ('data' in title_lower or 'ml' in title_lower):\n",
    "        return 'Data/ML Engineer'\n",
    "    elif 'machine learning' in title_lower or 'ml ' in title_lower:\n",
    "        return 'Machine Learning'\n",
    "    elif 'business intelligence' in title_lower or 'bi ' in title_lower:\n",
    "        return 'Business Intelligence'\n",
    "    elif 'ai' in title_lower or 'artificial intelligence' in title_lower:\n",
    "        return 'AI Specialist'\n",
    "    else:\n",
    "        return 'Otros'\n",
    "\n",
    "df['role_category'] = df['title'].apply(categorize_role)\n",
    "print(\"\\nüíº Distribuci√≥n de categor√≠as de roles:\")\n",
    "print(df['role_category'].value_counts())\n",
    "\n",
    "# %%\n",
    "# 2.5 Procesar salarios\n",
    "# Convertir a num√©rico\n",
    "df['salary_min'] = pd.to_numeric(df['salary_min'], errors='coerce')\n",
    "df['salary_max'] = pd.to_numeric(df['salary_max'], errors='coerce')\n",
    "\n",
    "# Calcular salario promedio\n",
    "df['salary_avg'] = (df['salary_min'] + df['salary_max']) / 2\n",
    "\n",
    "# Filtrar salarios realistas (entre 18K y 150K)\n",
    "df.loc[(df['salary_avg'] < 18000) | (df['salary_avg'] > 150000), 'salary_avg'] = np.nan\n",
    "\n",
    "print(f\"\\nüí∞ Ofertas con salario v√°lido: {df['salary_avg'].notna().sum()} ({df['salary_avg'].notna().sum()/len(df)*100:.1f}%)\")\n",
    "print(f\"üí∂ Salario promedio: {df['salary_avg'].mean():,.0f}‚Ç¨\")\n",
    "print(f\"üí∂ Mediana salarial: {df['salary_avg'].median():,.0f}‚Ç¨\")\n",
    "\n",
    "# %%\n",
    "# 2.6 Procesar fechas\n",
    "df['created'] = pd.to_datetime(df['created'], errors='coerce')\n",
    "df['collected_at'] = pd.to_datetime(df['collected_at'], errors='coerce')\n",
    "\n",
    "# Extraer componentes de fecha\n",
    "df['created_year'] = df['created'].dt.year\n",
    "df['created_month'] = df['created'].dt.month\n",
    "df['created_week'] = df['created'].dt.isocalendar().week\n",
    "\n",
    "print(\"\\nüìÖ Rango de fechas de publicaci√≥n:\")\n",
    "print(f\"Desde: {df['created'].min()}\")\n",
    "print(f\"Hasta: {df['created'].max()}\")\n",
    "\n",
    "# %%\n",
    "# 2.7 Limpiar nombres de empresas\n",
    "def clean_company_name(company):\n",
    "    \"\"\"Limpia nombres de empresas\"\"\"\n",
    "    if pd.isna(company):\n",
    "        return 'No especificada'\n",
    "    \n",
    "    company = str(company).strip()\n",
    "    \n",
    "    # Eliminar sufijos comunes\n",
    "    suffixes = [' S.L.', ' S.A.', ' SL', ' SA', ' Ltd', ' Inc', ' Corp']\n",
    "    for suffix in suffixes:\n",
    "        company = company.replace(suffix, '')\n",
    "    \n",
    "    return company.strip()\n",
    "\n",
    "df['company_clean'] = df['company_name'].apply(clean_company_name)\n",
    "print(f\"\\nüè¢ Empresas √∫nicas: {df['company_clean'].nunique()}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3Ô∏è‚É£ Extracci√≥n de Skills\n",
    "\n",
    "# %%\n",
    "# Lista de skills a buscar (ampliada)\n",
    "SKILLS = {\n",
    "    # Lenguajes\n",
    "    'Python': r'\\bpython\\b',\n",
    "    'R': r'\\b r\\b|\\br programming\\b',\n",
    "    'SQL': r'\\bsql\\b',\n",
    "    'Java': r'\\bjava\\b',\n",
    "    'Scala': r'\\bscala\\b',\n",
    "    \n",
    "    # ML/DL\n",
    "    'TensorFlow': r'\\btensorflow\\b',\n",
    "    'PyTorch': r'\\bpytorch\\b',\n",
    "    'Keras': r'\\bkeras\\b',\n",
    "    'scikit-learn': r'\\bscikit.learn\\b|\\bsklearn\\b',\n",
    "    'XGBoost': r'\\bxgboost\\b',\n",
    "    \n",
    "    # Big Data\n",
    "    'Spark': r'\\bspark\\b',\n",
    "    'Hadoop': r'\\bhadoop\\b',\n",
    "    'Kafka': r'\\bkafka\\b',\n",
    "    'Airflow': r'\\bairflow\\b',\n",
    "    \n",
    "    # Databases\n",
    "    'PostgreSQL': r'\\bpostgresql\\b|\\bpostgres\\b',\n",
    "    'MySQL': r'\\bmysql\\b',\n",
    "    'MongoDB': r'\\bmongodb\\b',\n",
    "    'Elasticsearch': r'\\belasticsearch\\b',\n",
    "    \n",
    "    # Cloud\n",
    "    'AWS': r'\\baws\\b',\n",
    "    'Azure': r'\\bazure\\b',\n",
    "    'GCP': r'\\bgcp\\b|\\bgoogle cloud\\b',\n",
    "    \n",
    "    # BI Tools\n",
    "    'Tableau': r'\\btableau\\b',\n",
    "    'Power BI': r'\\bpower bi\\b|\\bpowerbi\\b',\n",
    "    'Looker': r'\\blooker\\b',\n",
    "    \n",
    "    # Otros\n",
    "    'Docker': r'\\bdocker\\b',\n",
    "    'Kubernetes': r'\\bkubernetes\\b|\\bk8s\\b',\n",
    "    'Git': r'\\bgit\\b',\n",
    "    'Pandas': r'\\bpandas\\b',\n",
    "    'NumPy': r'\\bnumpy\\b',\n",
    "}\n",
    "\n",
    "# %%\n",
    "# Extraer skills de las descripciones\n",
    "def extract_skills(description):\n",
    "    \"\"\"Extrae skills mencionadas en la descripci√≥n\"\"\"\n",
    "    if pd.isna(description):\n",
    "        return []\n",
    "    \n",
    "    description_lower = str(description).lower()\n",
    "    found_skills = []\n",
    "    \n",
    "    for skill, pattern in SKILLS.items():\n",
    "        if re.search(pattern, description_lower, re.IGNORECASE):\n",
    "            found_skills.append(skill)\n",
    "    \n",
    "    return found_skills\n",
    "\n",
    "print(\"üîç Extrayendo skills de las descripciones...\")\n",
    "df['skills'] = df['description'].apply(extract_skills)\n",
    "df['num_skills'] = df['skills'].apply(len)\n",
    "\n",
    "print(f\"‚úÖ Skills extra√≠das\")\n",
    "print(f\"üìä Promedio de skills por oferta: {df['num_skills'].mean():.1f}\")\n",
    "\n",
    "# %%\n",
    "# Skills m√°s demandadas\n",
    "from collections import Counter\n",
    "\n",
    "all_skills = [skill for skills_list in df['skills'] for skill in skills_list]\n",
    "skill_counts = Counter(all_skills)\n",
    "\n",
    "print(\"\\nüî• Top 15 skills m√°s demandadas:\")\n",
    "for skill, count in skill_counts.most_common(15):\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"{skill:20} {count:4} ofertas ({percentage:5.1f}%)\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4Ô∏è‚É£ Detecci√≥n de IA/ML\n",
    "\n",
    "# %%\n",
    "# Detectar menciones de IA/ML/GPT\n",
    "AI_KEYWORDS = [\n",
    "    'artificial intelligence', 'ai', 'machine learning', 'ml', \n",
    "    'deep learning', 'neural network', 'llm', 'gpt', 'chatgpt',\n",
    "    'generative ai', 'computer vision', 'nlp', 'natural language'\n",
    "]\n",
    "\n",
    "def has_ai_keywords(description):\n",
    "    \"\"\"Detecta si la oferta menciona IA/ML\"\"\"\n",
    "    if pd.isna(description):\n",
    "        return False\n",
    "    \n",
    "    description_lower = str(description).lower()\n",
    "    return any(keyword in description_lower for keyword in AI_KEYWORDS)\n",
    "\n",
    "df['is_ai_related'] = df['description'].apply(has_ai_keywords)\n",
    "ai_jobs = df['is_ai_related'].sum()\n",
    "\n",
    "print(f\"ü§ñ Ofertas relacionadas con IA/ML: {ai_jobs} ({ai_jobs/len(df)*100:.1f}%)\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5Ô∏è‚É£ Guardar Datos Limpios\n",
    "\n",
    "# %%\n",
    "# Seleccionar columnas finales\n",
    "columns_to_keep = [\n",
    "    'id', 'title', 'company_clean', 'city', 'role_category', 'seniority',\n",
    "    'salary_min', 'salary_max', 'salary_avg', 'description',\n",
    "    'created', 'skills', 'num_skills', 'is_ai_related',\n",
    "    'redirect_url', 'contract_type', 'contract_time'\n",
    "]\n",
    "\n",
    "df_clean = df[columns_to_keep].copy()\n",
    "\n",
    "# Renombrar columnas\n",
    "df_clean = df_clean.rename(columns={\n",
    "    'company_clean': 'company',\n",
    "    'redirect_url': 'url'\n",
    "})\n",
    "\n",
    "# Guardar\n",
    "output_path = Path(\"../data/processed/jobs_cleaned.csv\")\n",
    "df_clean.to_csv(output_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\nüíæ Datos limpios guardados: {output_path}\")\n",
    "print(f\"üìä Total: {len(df_clean)} ofertas\")\n",
    "print(f\"üìù Columnas: {len(df_clean.columns)}\")\n",
    "\n",
    "# %%\n",
    "# Resumen final\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ LIMPIEZA COMPLETADA\".center(70))\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìä Dataset final:\")\n",
    "print(f\"   ‚Ä¢ {len(df_clean)} ofertas √∫nicas\")\n",
    "print(f\"   ‚Ä¢ {df_clean['city'].nunique()} ciudades\")\n",
    "print(f\"   ‚Ä¢ {df_clean['company'].nunique()} empresas\")\n",
    "print(f\"   ‚Ä¢ {df_clean['salary_avg'].notna().sum()} ofertas con salario\")\n",
    "print(f\"   ‚Ä¢ {df_clean['is_ai_related'].sum()} ofertas de IA/ML\")\n",
    "print(f\"   ‚Ä¢ Promedio: {df_clean['num_skills'].mean():.1f} skills por oferta\")\n",
    "\n",
    "print(\"\\nüéØ Siguiente paso: An√°lisis exploratorio detallado (notebook 03)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
